# 1.LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS


# 2.Composing Parameter-Efficient Modules with Arithmetic Operations
用算数方法进行微调模块的融合
![](assets/Pasted%20image%2020241020130450.png)

# 3.Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?
均匀的相加，和通过学习的方式，确定每个lora的权重。
![](assets/Pasted%20image%2020241020130549.png)
# 4.LORAHUB: EFFICIENT CROSS-TASK GENERALIZATION VIA DYNAMIC LORA COMPOSITION
![](assets/Pasted%20image%2020241020154615.png)
这里的 合成使用的是无梯度算法 来优化权重$w$
![](assets/Pasted%20image%2020241020155518.png)（CMA-ES）
# 5.LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin
图1，表示，指令微调SFT之后，模型在下游任务上的性能会提升。但是在QA问答任务上，性能会下降。就是忘记了原本预训练时的一些World Knowledge。
![](assets/Pasted%20image%2020241020164848.png)
图3，CBQA：闭卷问答。
![](assets/Pasted%20image%2020241020164810.png)
总结：
SFT 训练可以显著提升模型在 CBQA 数据集上的性能，尤其是在训练样本数量较少的情况下。
然而，随着训练样本数量的增加，性能提升逐渐放缓，并最终趋于稳定。
这说明模型在 CBQA 任务上的性能主要依赖于预训练阶段学习到的知识和技能。

MoE对比LoRA MoE
![](assets/Pasted%20image%2020241020182520.png)
# 6.LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design
在效率上做文章：解码延迟比其他动态适配器方法降低了 2.4 倍以上。

通用的混合专家MoE，搭配LoRA使用，提高模型的通用能力
![](assets/Pasted%20image%2020241020184648.png)
导致推理时间的增加。在效率上做文章。
![](assets/Pasted%20image%2020241025170145.png)
LoRA-Switch 的主要贡献：
揭示了动态适配器带来的高延迟开销问题，并分析了其根本原因，为后续优化提供了思路。
提出了 LoRA-Switch 架构，通过基于 token 的路由机制和融合的 CUDA 内核，有效地降低了推理延迟，同时保持了模型的高精度。
通过大量实验证明了 LoRA-Switch 的有效性，在通用和特定领域任务上都取得了优异的性能，并显著降低了解码延迟。
总结来说，LoRA-Switch 为动态适配器的高效应用提供了一种新的思路，并有望推动 LLM 的进一步发展和应用。

# 7.MIXTURE OF LORA EXPERTS
说明线性合成方法的不足：
1.多个lora直接相加，会顺海预训练模型的生成性能。≥3
2.归一化，可能消除单个lora的独特特征。每个lora的组合权重减少。

手工方法：不够灵活。
![](assets/Pasted%20image%2020241028214954.png)
如何动态有效地组合多个经过训练的lora，同时保留它们所有的个体特征？
本文的Mixture of LoRA Experts (MOLE)，用门控网络来对lora的每层都生成权重的分布。
![](assets/Pasted%20image%2020241028215214.png)
不归一化的时候，lora数量一多，破坏了预训练模型的参数。
归一化，降低了每个lora的特性。
这里也提到了PEMS(定义运算符)，和LoRAhub，无梯度算法。 图像领域提到了SVDiff
![](assets/Pasted%20image%2020241028215530.png)
本文的方法，每个lora是之前就训练好的，只训练门控函数，学习权重分布。
而且是每层学习一套权重
![](assets/Pasted%20image%2020241028220721.png)
提出了一个正则项函数，用于约束权重，使其偏向于均匀分配，不会全部集中到某一个上。



# 8.Merging Models with Fisher-Weighted Averaging
回顾fisher，判别式分类器。Fisher判别分析
Fisher线性判别——降维
![](assets/Pasted%20image%2020241028222129.png)

本文，Laplace 近似，

我们将合并与标准的基于梯度的迁移学习进行比较，并证明合并支持跨模型转移能力的根本不同的方法。具体来说，我们表明Fisher合并在中间任务训练和领域自适应预训练中与基于梯度的迁移学习方法竞争（同时明显便宜）。

# 9.Multi-LoRA Composition for Image Generation

